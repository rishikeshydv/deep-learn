{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datasets \n",
    "src_vocab = {\"<pad>\": 0, \"<eos>\": 1, \"<unk>\": 2, \"How\": 3, \"are\": 4, \"you\": 5, \"?\": 6}\n",
    "tgt_vocab = {\"<pad>\": 0, \"<eos>\": 1, \"<unk>\": 2, \"Comment\": 3, \"allez-vous\": 4, \"?\": 5}\n",
    "\n",
    "\n",
    "src_sentence = [\"How\", \"are\", \"you\", \"?\"]\n",
    "tgt_sentence = [\"Comment\", \"allez-vous\", \"?\"]\n",
    "\n",
    "src_indices = [src_vocab[word] for word in src_sentence]\n",
    "tgt_indices = [tgt_vocab[word] for word in tgt_sentence]\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "embedding_dim = 64\n",
    "\n",
    "src_embedding = nn.Embedding(src_vocab_size, embedding_dim)\n",
    "tgt_embedding = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "src_indices_tensor = torch.tensor([src_indices], dtype=torch.long)  \n",
    "tgt_indices_tensor = torch.tensor([tgt_indices], dtype=torch.long)\n",
    "\n",
    "src_mask = (src_indices_tensor != src_vocab[\"<pad>\"])\n",
    "tgt_mask = (tgt_indices_tensor != tgt_vocab[\"<pad>\"])\n",
    "\n",
    "src_embeddings = src_embedding(src_indices_tensor)  \n",
    "tgt_embeddings = tgt_embedding(tgt_indices_tensor) \n",
    "\n",
    "Q = src_embeddings \n",
    "K = tgt_embeddings \n",
    "V = tgt_embeddings \n",
    "\n",
    "if K.shape[1] < Q.shape[1]:\n",
    "    pad_length = Q.shape[1] - K.shape[1]\n",
    "    K = torch.nn.functional.pad(K, (0, 0, 0, pad_length))  \n",
    "    V= torch.nn.functional.pad(V, (0, 0, 0, pad_length))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RLAttention(nn.Module):\n",
    "    def __init__(self, model_dimension, n_heads):\n",
    "        super(RLAttention, self).__init__()\n",
    "        self.model_dimension = model_dimension\n",
    "        self.n_heads = n_heads\n",
    "        self.dimension = model_dimension // n_heads\n",
    "        self.seq_length = K.size(1)\n",
    "\n",
    "        self.q = nn.Linear(model_dimension, model_dimension)\n",
    "        self.k = nn.Linear(model_dimension, model_dimension)\n",
    "        self.v = nn.Linear(model_dimension, model_dimension)\n",
    "        self.o = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(self.dimension, 64),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.seq_length),  \n",
    "            nn.Softmax(dim=-1)  \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.q(Q))\n",
    "        K = self.split_heads(self.k(K))\n",
    "        V = self.split_heads(self.v(V))\n",
    "        \n",
    "        attention_weights = self.select_attention_weights(Q, K, mask)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        combined = self.combine_heads(output)\n",
    "        return self.o(combined), attention_weights\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, model_dim = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.n_heads, self.dimension)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, n_heads, seq_length, dimension = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(batch_size, seq_length, n_heads * dimension)\n",
    "\n",
    "    def select_attention_weights(self, Q, K, mask=None):\n",
    "        batch_size, n_heads, seq_length_query, dimension = Q.size()\n",
    "        _,_,seq_length_key, _ = K.size()\n",
    "\n",
    "        # convert the queries to 1D for policy input\n",
    "        Q_flat = Q.reshape(batch_size * n_heads * seq_length_query, dimension)\n",
    "        \n",
    "        action_probs = self.policy_network(Q_flat)\n",
    "        \n",
    "        action_probabilities = action_probs.view(batch_size, n_heads, seq_length_query, seq_length_key)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2) \n",
    "            action_probabilities = action_probabilities.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(action_probabilities, dim=-1)\n",
    "\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RLAttention(model_dimension=64, n_heads=8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(predicted_translation, target_translation):\n",
    "    return torch.cosine_similarity(predicted_translation, target_translation, dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.1702141761779785\n",
      "Epoch 2/20, Loss: -6.235805988311768\n",
      "Epoch 3/20, Loss: -16.6051082611084\n",
      "Epoch 4/20, Loss: -26.24207305908203\n",
      "Epoch 5/20, Loss: -34.75881576538086\n",
      "Epoch 6/20, Loss: -42.0794677734375\n",
      "Epoch 7/20, Loss: -48.25393295288086\n",
      "Epoch 8/20, Loss: -53.37934494018555\n",
      "Epoch 9/20, Loss: -57.57787322998047\n",
      "Epoch 10/20, Loss: -60.98817443847656\n",
      "Epoch 11/20, Loss: -63.745872497558594\n",
      "Epoch 12/20, Loss: -65.97418212890625\n",
      "Epoch 13/20, Loss: -67.77554321289062\n",
      "Epoch 14/20, Loss: -69.23640441894531\n",
      "Epoch 15/20, Loss: -70.4271469116211\n",
      "Epoch 16/20, Loss: -71.40367889404297\n",
      "Epoch 17/20, Loss: -72.21573638916016\n",
      "Epoch 18/20, Loss: -72.90322875976562\n",
      "Epoch 19/20, Loss: -73.50045776367188\n",
      "Epoch 20/20, Loss: -74.03679656982422\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = model(Q, K, V, mask=src_mask)\n",
    "    \n",
    "    rewards = reward_function(output, src_embeddings)\n",
    " \n",
    "    log_probs = torch.log(attention_weights + 1e-9) \n",
    "    loss = -torch.sum(log_probs * rewards.unsqueeze(-1))\n",
    "    \n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
