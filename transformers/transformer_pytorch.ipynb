{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "NmNGyrCTLNwP"
      },
      "outputs": [],
      "source": [
        "#pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "ZOwOZCvr_Gad"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "myRJi-ro_gq4"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dimension, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert model_dimension % n_heads == 0\n",
        "        self.dimension = model_dimension // n_heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Linear layers for Q, K, V, and output\n",
        "        self.q = nn.Linear(model_dimension, model_dimension)\n",
        "        self.k = nn.Linear(model_dimension, model_dimension)\n",
        "        self.v = nn.Linear(model_dimension, model_dimension)\n",
        "        self.o = nn.Linear(model_dimension, model_dimension)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dimension)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, model_dim = x.size()\n",
        "        x = x.view(batch_size, seq_length, self.n_heads, self.dimension)\n",
        "        return x.permute(0, 2, 1, 3)  # (batch_size, n_heads, seq_length, dimension)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, n_heads, seq_length, dimension = x.size()\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        return x.view(batch_size, seq_length, n_heads * dimension)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.q(Q))\n",
        "        K = self.split_heads(self.k(K))\n",
        "        V = self.split_heads(self.v(V))\n",
        "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        combined = self.combine_heads(attention)\n",
        "        return self.o(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "aM0zgKPo9kHH"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, model_dimension, ff_dimension):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(model_dimension, ff_dimension)\n",
        "        self.fc2 = nn.Linear(ff_dimension, model_dimension)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "mU8H2ocsAODv"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dimension, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_seq_length, model_dimension)\n",
        "        position = torch.arange(0, max_seq_length).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, model_dimension, 2).float() * -(math.log(10000.0) / model_dimension))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "tsAXolfdFA4P"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, n_heads, ff_dimension, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dimension, n_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(model_dimension, ff_dimension)\n",
        "        self.norm1 = nn.LayerNorm(model_dimension)\n",
        "        self.norm2 = nn.LayerNorm(model_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpPZCsbJG9-f"
      },
      "source": [
        "Now, we will be working on the decoder blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "h7N9RfnOHEGX"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, n_heads, ff_dimension, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(model_dimension, n_heads)\n",
        "        self.cross_attn = MultiHeadAttention(model_dimension, n_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(model_dimension, ff_dimension)\n",
        "        self.norm1 = nn.LayerNorm(model_dimension)\n",
        "        self.norm2 = nn.LayerNorm(model_dimension)\n",
        "        self.norm3 = nn.LayerNorm(model_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        return self.norm3(x + self.dropout(ff_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXmFj_-KxF-"
      },
      "source": [
        "Combining the Encoder and Decoder layers to create the complete Transformer network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "PV0aWgxlK0qN"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, model_dimension, n_heads, num_layers, ff_dimension, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, model_dimension)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, model_dimension)\n",
        "        self.positional_encoding = PositionalEncoding(model_dimension, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(model_dimension, n_heads, ff_dimension, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(model_dimension, n_heads, ff_dimension, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(model_dimension, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(tgt.device)\n",
        "        tgt_mask = tgt_mask & ~nopeak_mask.unsqueeze(0)\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc(dec_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_seq_length):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.encode(self.src_sentences[idx], self.src_vocab)\n",
        "        tgt = self.encode(self.tgt_sentences[idx], self.tgt_vocab)\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "    def encode(self, sentence, vocab):\n",
        "        tokens = [vocab.get(token, vocab[\"<unk>\"]) for token in sentence.split()]\n",
        "        tokens = tokens[:self.max_seq_length - 1] + [vocab[\"<eos>\"]]\n",
        "        return tokens + [vocab[\"<pad>\"]] * (self.max_seq_length - len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvxRuhg6ObrN"
      },
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "BnO4-Cm-Oduk"
      },
      "outputs": [],
      "source": [
        "\n",
        "src_vocab = {\"<pad>\": 0, \"<eos>\": 1, \"<unk>\": 2, \"How\": 3, \"are\": 4, \"you\": 5, \"?\": 6}\n",
        "tgt_vocab = {\"<pad>\": 0, \"<eos>\": 1, \"<unk>\": 2, \"Comment\": 3, \"allez-vous\": 4, \"?\": 5}\n",
        "\n",
        "# Example sentences\n",
        "src_sentences = [\"How are you?\", \"What is your name?\"]\n",
        "tgt_sentences = [\"Comment allez-vous?\", \"Quel est votre nom?\"]\n",
        "\n",
        "transformer = Transformer(\n",
        "    src_vocab_size=len(src_vocab),\n",
        "    tgt_vocab_size=len(tgt_vocab),\n",
        "    model_dimension=512,\n",
        "    n_heads=8,\n",
        "    num_layers=6,\n",
        "    ff_dimension=2048,\n",
        "    max_seq_length=10,\n",
        "    dropout=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_seq_length=10)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "wUAsUY-5O7E2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.7579504251480103\n",
            "Epoch 2, Loss: 1.4884840250015259\n",
            "Epoch 3, Loss: 0.5105863213539124\n",
            "Epoch 4, Loss: 1.0654737949371338\n",
            "Epoch 5, Loss: 0.5923992991447449\n",
            "Epoch 6, Loss: 0.6513323187828064\n",
            "Epoch 7, Loss: 0.8769221305847168\n",
            "Epoch 8, Loss: 0.7369091510772705\n",
            "Epoch 9, Loss: 0.48680350184440613\n",
            "Epoch 10, Loss: 0.5094538331031799\n",
            "Time taken: 1.1457328796386719 seconds\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "#start time\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    epoch_loss = 0\n",
        "    for src_batch, tgt_batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Remove the last token from the target for input and first for target\n",
        "        src, tgt_input, tgt_output = src_batch, tgt_batch[:, :-1], tgt_batch[:, 1:]\n",
        "\n",
        "        # Forward pass\n",
        "        output = transformer(src, tgt_input)\n",
        "        loss = criterion(output.reshape(-1, len(tgt_vocab)), tgt_output.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(data_loader)}\")\n",
        "\n",
        "#end time\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Time taken: {end-start} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
